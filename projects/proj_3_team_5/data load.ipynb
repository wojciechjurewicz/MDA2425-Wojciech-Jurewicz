{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add paths for custom transformers\n",
    "while any(marker in os.getcwd() for marker in ('exercises', 'notebooks', 'students', 'research', 'projects')):\n",
    "    os.chdir(\"..\")\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from src.custom_transformers import (\n",
    "    CustomOneHotEncoder,\n",
    "    CustomStandardScaler\n",
    ")\n",
    "import projects.proj_3_team_5.src.custom_transformers as project_transformers\n",
    "from projects.proj_3_team_5.src.custom_transformers import CustomTokenizerVectorizer, ToDataFrameFromColumnTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = 'projects/proj_3_team_5/.env'\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ruthgn/beer-profile-and-ratings-data-set\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_path = os.getenv('RAW_DATA_DIR')\n",
    "df_preprocessed_path = os.getenv('PREPROCESSED_DATA_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct path to your raw data file if not already set\n",
    "if not df_raw_path:\n",
    "\t# Example: update the filename as needed\n",
    "\tdf_raw_path = os.path.join(path, \"beer_profile_and_ratings.csv\")\n",
    "\n",
    "df_raw = pd.read_csv(df_raw_path)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values:\")\n",
    "missing = df_raw.isnull().sum()\n",
    "print(missing[missing > 0].sort_values(ascending=False))\n",
    "\n",
    "# Add empty cells check\n",
    "empty_cells = df_raw.eq('').sum()\n",
    "print(\"\\nEmpty String Values:\")\n",
    "print(empty_cells[empty_cells > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dupes = df_raw.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows in train data: {num_dupes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cardinality of categorical columns\n",
    "categorical_cols = df_raw.select_dtypes(include=['object']).columns\n",
    "cardinality = {col: df_raw[col].nunique() for col in categorical_cols}\n",
    "print(\"Number of unique values in categorical columns:\")\n",
    "for col, count in sorted(cardinality.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all descriptions start with \"Notes:\"\n",
    "all_start_with_notes = df_raw['Description'].str.startswith('Notes:').all()\n",
    "print(f\"All descriptions start with 'Notes:': {all_start_with_notes}\")\n",
    "\n",
    "# Show some examples if not all start with \"Notes:\"\n",
    "if not all_start_with_notes:\n",
    "    print(\"\\nExamples of descriptions that don't start with 'Notes:':\")\n",
    "    non_notes = df_raw[~df_raw['Description'].str.startswith('Notes:')]['Description'].head()\n",
    "    print(non_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric columns\n",
    "numeric_cols = df_raw.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"Numeric columns:\")\n",
    "print(numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pipeline = make_pipeline(\n",
    "    project_transformers.TextCleaner(\n",
    "        column='Description',\n",
    "        patterns_to_remove=['^Notes:', 'error entering this description']\n",
    "    ),\n",
    "    project_transformers.CustomOutlierRemover(\n",
    "        columns=numeric_cols,\n",
    "        threshold=3\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = cleaning_pipeline.fit_transform(df_raw)\n",
    "\n",
    "print(\"Original shape:\", df_raw.shape)\n",
    "print(\"Processed shape:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count empty cells in Description column\n",
    "empty_descriptions = df_cleaned['Description'].isna().sum() + (df_cleaned['Description'] == '').sum()\n",
    "print(f\"Number of empty cells in Description: {empty_descriptions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_path = os.getenv('CLEANED_DATA_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(df_cleaned_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers=ColumnTransformer([\n",
    "            ('name_tok', CustomTokenizerVectorizer(column='Name', vocab_size=300), ['Name']),\n",
    "            ('desc_tok', CustomTokenizerVectorizer(column='Description', vocab_size=1000), ['Description']),\n",
    "            ('beer_tok', CustomTokenizerVectorizer(column='Beer Name (Full)', vocab_size=500), ['Beer Name (Full)']),\n",
    "        ],\n",
    "        remainder='passthrough', sparse_threshold=0, \n",
    "        verbose_feature_names_out=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = make_pipeline(\n",
    "    transformers,\n",
    "    ToDataFrameFromColumnTransformer(transformers),\n",
    "    CustomOneHotEncoder(\n",
    "        columns=['Style', 'Brewery'],\n",
    "    ),\n",
    "    CustomStandardScaler(\n",
    "        columns=numeric_cols\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = final_pipeline.fit_transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.to_csv(df_preprocessed_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
