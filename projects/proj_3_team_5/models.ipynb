{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d643b21b",
   "metadata": {},
   "source": [
    "# Introduction - clustering analysis of beer review dataset\n",
    "\n",
    "This notebook performs an in-depth clustering analysis on a dataset containing various beer characteristics, including sensory ratings, chemical properties, and user reviews. The goal is to identify meaningful groupings of beers based on different subsets of features.\n",
    "\n",
    "### Key Steps Covered:\n",
    "1. **Optimal Parameter Selection**  \n",
    "   - For **KMeans**, the Elbow Method and Silhouette Score are used to determine the best number of clusters.  \n",
    "   - For **Gaussian Mixture Models (GMM)**, the Bayesian Information Criterion (BIC) is used.  \n",
    "   - For **DBSCAN**, the k-distance graph helps estimate an appropriate epsilon.\n",
    "\n",
    "2. **Model Comparison**  \n",
    "   - The following clustering models are tested:  \n",
    "     *KMeans*, *GaussianMixture*, *DBSCAN*, *HDBSCAN*, *MeanShift*, *AgglomerativeClustering*.  \n",
    "   - Each model is evaluated using:  \n",
    "     - **Silhouette Score** (cluster separation)\n",
    "     - **Calinski-Harabasz Score** (cluster compactness)\n",
    "     - **Davies-Bouldin Score** (cluster similarity)\n",
    "\n",
    "3. **Visualization**  \n",
    "   - **PCA** is used to project high-dimensional data into 2D space for visual inspection.  \n",
    "   - **Radar plots** visualize average feature values across clusters for:\n",
    "     - Full feature set\n",
    "     - Feature subsets: *Sensory*, *Profile*, *Chemical*, *Reviews*\n",
    "\n",
    "4. **Export**  \n",
    "   - Radar plots are saved to *projects/proj_3_team_5/plots/* for reporting and further analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeabfc5",
   "metadata": {},
   "source": [
    "# Importing and data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d46c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, HDBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from kneed import KneeLocator\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "while any(marker in os.getcwd() for marker in ('exercises', 'notebooks', 'students', 'research', 'projects')):\n",
    "    os.chdir(\"..\") \n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92de8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('projects/proj_3_team_5/.env')\n",
    "df_path = os.getenv('PREPROCESSED_DATA_DIR')\n",
    "df_cleaned_path = os.getenv('CLEANED_DATA_DIR')\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "df_raw = pd.read_csv(df_cleaned_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436af9b",
   "metadata": {},
   "source": [
    "# Optimal parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For KMeans, we use the Elbow method (inertia) and the Silhouette Score to select the optimal number of clusters.\n",
    "# - The Elbow method helps identify the point where adding more clusters does not significantly reduce the within-cluster sum of squares (inertia), indicating a suitable number of clusters.\n",
    "# - The Silhouette Score measures how similar an object is to its own cluster compared to other clusters, providing a quantitative metric for cluster quality.\n",
    "# For GaussianMixture, we use the Bayesian Information Criterion (BIC) to select the optimal number of components.\n",
    "# - BIC penalizes model complexity while rewarding goodness of fit, making it suitable for model selection in probabilistic clustering like GMM.\n",
    "# - Lower BIC values indicate a better model, balancing fit and complexity.\n",
    "inertia = []\n",
    "silhouette = []\n",
    "bic = []\n",
    "n_range = range(2, 11)\n",
    "\n",
    "for n in n_range:\n",
    "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "    labels = kmeans.fit_predict(df)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette.append(silhouette_score(df, labels))\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42)\n",
    "    gmm_labels = gmm.fit_predict(df)\n",
    "    bic.append(gmm.bic(df))\n",
    "\n",
    "# Plot KMeans elbow (inertia) and silhouette\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(n_range, inertia, marker='o')\n",
    "plt.title('KMeans Elbow Method (Inertia)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(n_range, silhouette, marker='o')\n",
    "plt.title('KMeans Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot GMM BIC\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(n_range, bic, marker='o')\n",
    "plt.title('GaussianMixture BIC')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('BIC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For DBSCAN, we use the k-distance graph to estimate the optimal value of eps (the neighborhood radius).\n",
    "# - The k-distance plot helps visualize the distance to the k-th nearest neighbor for each point, sorted in ascending order.\n",
    "# - The \"elbow\" in this plot suggests a threshold where points start to become outliers, which is a good candidate for eps.\n",
    "\n",
    "k = 7\n",
    "neigh = NearestNeighbors(n_neighbors=k)\n",
    "nbrs = neigh.fit(df)\n",
    "distances, indices = nbrs.kneighbors(df)\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_distances)\n",
    "plt.title('DBSCAN k-distance Graph')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel(f'{k}th Nearest Neighbor Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bc3d1",
   "metadata": {},
   "source": [
    "Based on the KMeans parameter tuning:\n",
    "\n",
    "- The **Elbow Method** shows a visible bend around $k = 8$, suggesting diminishing returns in inertia reduction beyond this point.\n",
    "- However, the **Silhouette Score** is highest at $k = 2$ (approximately $0.12$), and drops sharply for higher values of $k$, even becoming negative, which indicates poor cluster separation.\n",
    "\n",
    "Therefore, considering both methods, the optimal number of clusters is: $k=2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f412c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_kmeans = 2\n",
    "optimal_gmm = 2\n",
    "optimal_eps = 1250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690f763",
   "metadata": {},
   "source": [
    "For Agglomerative Clustering, we use the same number of clusters as determined optimal for KMeans, since both are hierarchical/partitioning methods and can be compared directly.\n",
    "\n",
    "For MeanShift and HDBSCAN, we use their default parameter estimation, as these algorithms are designed to infer the number of clusters or density structure from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca38404",
   "metadata": {},
   "source": [
    "# PCA visualization and evaluation of clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'KMeans': KMeans(n_clusters=optimal_kmeans, random_state=42),\n",
    "    'GaussianMixture': GaussianMixture(n_components=optimal_gmm, covariance_type='full', random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=optimal_eps, min_samples=k),\n",
    "    'HDBSCAN': HDBSCAN(),\n",
    "    'MeanShift': MeanShift(),\n",
    "    'Agglomerative': AgglomerativeClustering(n_clusters=optimal_kmeans, linkage='complete')\n",
    "}\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(df)\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, (name, model) in enumerate(models.items(), 1):\n",
    "    try:\n",
    "        labels = model.fit_predict(df)\n",
    "    except Exception as e:\n",
    "        labels = np.zeros(df.shape[0])\n",
    "        print(f\"Model {name} failed: {e}\")\n",
    "\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=10)\n",
    "    plt.title(f'{name} Clustering')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    if n_clusters < 2:\n",
    "        sil = -1\n",
    "        ch = -1\n",
    "        db = np.inf\n",
    "    else:\n",
    "        sil = silhouette_score(df, labels)\n",
    "        ch = calinski_harabasz_score(df, labels)\n",
    "        db = davies_bouldin_score(df, labels)\n",
    "    metrics[name] = {\n",
    "        'silhouette': sil,\n",
    "        'calinski_harabasz': ch,\n",
    "        'davies_bouldin': db,\n",
    "        'n_clusters': n_clusters\n",
    "    }\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9aff2d",
   "metadata": {},
   "source": [
    "# Selection of best clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f413466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensory_cols = ['review_aroma', 'review_appearance', 'review_palate', 'review_taste', 'review_overall']\n",
    "profile_cols = ['Alcohol', 'Bitter', 'Sweet', 'Sour', 'Salty', 'Fruits', 'Hoppy', 'Spices', 'Malty', 'Astringency', 'Body']\n",
    "chemical_cols = ['ABV', 'Min IBU', 'Max IBU']\n",
    "review_cols = ['number_of_reviews']\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "display(metrics_df)\n",
    "\n",
    "# Choose the best clustering based on silhouette score (higher is better)\n",
    "valid_metrics = metrics_df[metrics_df['n_clusters'] > 1]\n",
    "if not valid_metrics.empty:\n",
    "    best_model_name = valid_metrics['silhouette'].idxmax()\n",
    "    print(f\"Best clustering model: {best_model_name}\")\n",
    "else:\n",
    "    best_model_name = metrics_df['silhouette'].idxmax()\n",
    "    print(f\"Best clustering model (by default): {best_model_name}\")\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "best_labels = best_model.fit_predict(df)\n",
    "\n",
    "# Radar plot for the best clustering\n",
    "radar_cols = sensory_cols + profile_cols + chemical_cols + review_cols\n",
    "radar_cols = [col for col in radar_cols if col in df.columns]\n",
    "\n",
    "# Compute mean for each cluster\n",
    "cluster_means = pd.DataFrame(df[radar_cols])\n",
    "cluster_means['cluster'] = best_labels\n",
    "cluster_means = cluster_means.groupby('cluster').mean()\n",
    "\n",
    "# Prepare radar plot\n",
    "categories = radar_cols\n",
    "N = len(categories)\n",
    "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "angles += angles[:1]  # close the loop\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for idx, (cluster, row) in enumerate(cluster_means.iterrows()):\n",
    "    values = row.values.flatten().tolist()\n",
    "    values += values[:1]  # close the loop\n",
    "    plt.polar(angles, values, label=f'Cluster {cluster}', linewidth=2)\n",
    "\n",
    "plt.xticks(angles[:-1], categories, color='grey', size=10)\n",
    "plt.title(f'Radar Plot of Cluster Means for {best_model_name}', size=15, y=1.08)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"projects/proj_3_team_5/plots/radar_{best_model_name}_overall.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_with_clusters = df_raw.copy()\n",
    "df_with_clusters['cluster'] = best_labels\n",
    "\n",
    "# Show five sample beers from each cluster only if number of clusters is smaller than 10\n",
    "n_clusters = len(df_with_clusters['cluster'].unique())\n",
    "if n_clusters < 10:\n",
    "    print(f\"\\n=== Sample Beers from Each Cluster ({best_model_name}) ===\")\n",
    "    for cluster in sorted(df_with_clusters['cluster'].unique()):\n",
    "        cluster_beers = df_with_clusters[df_with_clusters['cluster'] == cluster]\n",
    "        print(f\"\\nCluster {cluster} ({len(cluster_beers)} beers):\")\n",
    "        \n",
    "        # Sample 5 beers from this cluster\n",
    "        sample_beers = cluster_beers.sample(n=min(5, len(cluster_beers)), random_state=42)\n",
    "        display(sample_beers)\n",
    "else:\n",
    "    print(f\"\\nSkipping sample display - too many clusters ({n_clusters})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ce6bb",
   "metadata": {},
   "source": [
    "# Clustering analysis and visualization by feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = {\n",
    "    'Sensory': sensory_cols,\n",
    "    'Profile': profile_cols,\n",
    "    'Chemical': chemical_cols,\n",
    "    'Reviews': review_cols\n",
    "}\n",
    "\n",
    "for group_name, columns in feature_groups.items():\n",
    "    metrics = {}\n",
    "    print(f\"\\n=== Feature Group: {group_name} ===\")\n",
    "    print(\"Clustering results for each model:\")\n",
    "    X = df[columns].dropna()\n",
    "    n_components = min(2, X.shape[0], X.shape[1])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.suptitle(f'Feature Group: {group_name}', fontsize=16)\n",
    "\n",
    "    # Find optimal parameters for the models using the elbow method\n",
    "\n",
    "    # KMeans: Find optimal number of clusters using elbow and silhouette score\n",
    "    sse = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, min(11, X.shape[0]))\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        # Only compute silhouette if more than 1 cluster\n",
    "        if len(set(labels)) > 1:\n",
    "            sil = silhouette_score(X, labels)\n",
    "        else:\n",
    "            sil = -1\n",
    "        silhouette_scores.append(sil)\n",
    "    # Find elbow point (simple heuristic: where the decrease sharply slows)\n",
    "    if len(sse) > 2:\n",
    "        elbow_k = k_range[np.argmin(np.diff(sse, 2)) + 1]\n",
    "    else:\n",
    "        elbow_k = k_range[0]\n",
    "    # Find k with maximum silhouette score\n",
    "    best_sil_k = k_range[np.argmax(silhouette_scores)]\n",
    "    # Combine: pick k that is closest to elbow_k but also has high silhouette (within 90% of max)\n",
    "    sil_threshold = 0.9 * max(silhouette_scores)\n",
    "    candidate_ks = [k for k, sil in zip(k_range, silhouette_scores) if sil >= sil_threshold]\n",
    "    if candidate_ks:\n",
    "        optimal_kmeans = min(candidate_ks, key=lambda k: abs(k - elbow_k))\n",
    "    else:\n",
    "        optimal_kmeans = elbow_k\n",
    "\n",
    "    # GaussianMixture: Use same optimal number of components as KMeans\n",
    "    optimal_gmm = optimal_kmeans\n",
    "\n",
    "    # DBSCAN: Find optimal eps using k-distance graph (elbow method)\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=2)\n",
    "    nbrs = neigh.fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "    distances = np.sort(distances[:, 1])\n",
    "    # Heuristic: take the point of maximum curvature as optimal eps\n",
    "\n",
    "    kneedle = KneeLocator(range(len(distances)), distances, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
    "    optimal_eps = distances[kneedle.knee] if kneedle.knee is not None else np.percentile(distances, 90)\n",
    "    k = 2  # min_samples\n",
    "\n",
    "    # Agglomerative: Use same optimal number of clusters as KMeans\n",
    "    optimal_agglom = optimal_kmeans\n",
    "\n",
    "    models = {\n",
    "        'KMeans': KMeans(n_clusters=optimal_kmeans, random_state=42),\n",
    "        'GaussianMixture': GaussianMixture(n_components=optimal_gmm, covariance_type='full', random_state=42),\n",
    "        'DBSCAN': DBSCAN(eps=optimal_eps, min_samples=k),\n",
    "        'HDBSCAN': HDBSCAN(),\n",
    "        'MeanShift': MeanShift(),\n",
    "        'Agglomerative': AgglomerativeClustering(n_clusters=optimal_agglom, linkage='complete')\n",
    "    }\n",
    "\n",
    "\n",
    "    for i, (model_name, model) in enumerate(models.items(), 1):\n",
    "        labels = model.fit_predict(X)\n",
    "\n",
    "        plt.subplot(2, 3, i)\n",
    "        if n_components == 1:\n",
    "            plt.scatter(X_pca[:, 0], [0]*len(X_pca), c=labels, cmap='tab10', s=10)\n",
    "            plt.xlabel('PCA 1')\n",
    "            plt.ylabel('0 (no 2nd PCA component)')\n",
    "        else:\n",
    "            plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=10)\n",
    "            plt.xlabel('PCA 1')\n",
    "            plt.ylabel('PCA 2')\n",
    "\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        if n_clusters < 2:\n",
    "            sil = -1\n",
    "            ch = -1\n",
    "            db = np.inf\n",
    "        else:\n",
    "            sil = silhouette_score(X, labels)\n",
    "            ch = calinski_harabasz_score(X, labels)\n",
    "            db = davies_bouldin_score(X, labels)\n",
    "    \n",
    "        plt.title(f'{model_name}')\n",
    "        metrics[model_name] = {\n",
    "            'silhouette': sil,\n",
    "            'calinski_harabasz': ch,\n",
    "            'davies_bouldin': db,\n",
    "            'n_clusters': n_clusters\n",
    "        }\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Model metrics for this group:\")\n",
    "    metrics_df = pd.DataFrame(metrics).T\n",
    "    display(metrics_df)\n",
    "\n",
    "    # Plot radar plot for the best model in this group\n",
    "    valid_metrics = metrics_df[metrics_df['n_clusters'] > 1]\n",
    "    if not valid_metrics.empty:\n",
    "        best_model_name = valid_metrics['silhouette'].idxmax()\n",
    "    else:\n",
    "        best_model_name = metrics_df['silhouette'].idxmax()\n",
    "    print(f\"Best clustering model for {group_name}: {best_model_name}\")\n",
    "\n",
    "    best_model = models[best_model_name]\n",
    "    best_labels = best_model.fit_predict(X)\n",
    "\n",
    "    # Compute mean for each cluster\n",
    "    cluster_means = pd.DataFrame(X, columns=columns)\n",
    "    cluster_means['cluster'] = best_labels\n",
    "    cluster_means = cluster_means.groupby('cluster').mean()\n",
    "\n",
    "    # Prepare radar plot\n",
    "    categories = columns\n",
    "    N = len(categories)\n",
    "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # close the loop\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for idx, (cluster, row) in enumerate(cluster_means.iterrows()):\n",
    "        values = row.values.flatten().tolist()\n",
    "        values += values[:1]  # close the loop\n",
    "        plt.polar(angles, values, label=f'Cluster {cluster}', linewidth=2)\n",
    "\n",
    "    plt.xticks(angles[:-1], categories, color='grey', size=10)\n",
    "    plt.title(f'Radar Plot of Cluster Means for {best_model_name} ({group_name})', size=15, y=1.08)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"projects/proj_3_team_5/plots/radar__{best_model_name}_{group_name.lower()}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Add cluster labels to the original dataframe\n",
    "    df_with_clusters = df_raw.copy()\n",
    "    df_with_clusters['cluster'] = best_labels\n",
    "\n",
    "    # Show five sample beers from each cluster only if number of clusters is smaller than 10\n",
    "    n_clusters = len(df_with_clusters['cluster'].unique())\n",
    "    if n_clusters < 10:\n",
    "        print(f\"\\n=== Sample Beers from Each Cluster ({best_model_name}) ===\")\n",
    "        for cluster in sorted(df_with_clusters['cluster'].unique()):\n",
    "            cluster_beers = df_with_clusters[df_with_clusters['cluster'] == cluster]\n",
    "            print(f\"\\nCluster {cluster} ({len(cluster_beers)} beers):\")\n",
    "            \n",
    "            # Sample 5 beers from this cluster\n",
    "            sample_beers = cluster_beers.sample(n=min(5, len(cluster_beers)), random_state=42)\n",
    "            display(sample_beers)\n",
    "    else:\n",
    "        print(f\"\\nSkipping sample display - too many clusters ({n_clusters})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddbe07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
