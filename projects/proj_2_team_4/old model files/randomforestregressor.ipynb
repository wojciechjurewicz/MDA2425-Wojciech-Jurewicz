{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "while any(marker in os.getcwd() for marker in ('exercises', 'notebooks', 'students', 'research', 'projects')):\n",
    "    os.chdir(\"..\")\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wojciechjurewicz/anaconda3/envs/mda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import optuna\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_log_error # Metric used in the competition for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI to local directory\n",
    "mlflow.set_tracking_uri(\"projects/proj_2_team_4/mlruns\")\n",
    "\n",
    "# Use for running ui:\n",
    "# mlflow ui --backend-store-uri \"Absolute path\"\n",
    "# e.g.: mlflow ui --backend-store-uri \"/Users/wojciechjurewicz/Desktop/Multivariate Data Analysis/Lab/mda2425/projects/proj_2_team_4/mlruns\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from the correct .env file location\n",
    "env_path = 'projects/proj_2_team_4/.env'\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset path from environment variable\n",
    "train_preprocessed_path = os.getenv('TRAIN_PREPROCESSED_PATH')\n",
    "test_preprocessed_path = os.getenv('VALID_PREPROCESSED_PATH')\n",
    "\n",
    "df_train = pd.read_csv(train_preprocessed_path)\n",
    "df_test = pd.read_csv(test_preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"SalePrice\"])\n",
    "y_train = df_train[\"SalePrice\"]\n",
    "\n",
    "X_test = df_test.drop(columns=[\"SalePrice\"])\n",
    "y_test = df_test[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=300\n",
    "max_depth=20\n",
    "min_samples_split=5\n",
    "min_samples_leaf=2\n",
    "max_features=0.5\n",
    "bootstrap=True\n",
    "n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('RandomForestRegressor', RandomForestRegressor(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        min_samples_split=min_samples_split, \n",
    "        min_samples_leaf=min_samples_leaf, \n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "  \"\"\"\n",
    "  Logging callback that will report when a new trial iteration improves upon existing\n",
    "  best trial values.\n",
    "\n",
    "  Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "  or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "  workers or agents.\n",
    "  The race conditions with file system state management for distributed trials will render\n",
    "  inconsistent values with this callback.\n",
    "  \"\"\"\n",
    "\n",
    "  winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "  if study.best_value and winner != study.best_value:\n",
    "      study.set_user_attr(\"winner\", study.best_value)\n",
    "      if winner:\n",
    "          improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "          print(\n",
    "              f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "              f\"{improvement_percent: .4f}% improvement\"\n",
    "          )\n",
    "      else:\n",
    "          print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 12),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0.1, 1.0),  # Fraction of features\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "}\n",
    "    \n",
    "    # Create and train pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('RandomForestRegressor', RandomForestRegressor(**params))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    y_pred = np.maximum(0, y_pred) # Added to fix RMSLE ValueError\n",
    "    \n",
    "    # Return RMSLE as optimization metric\n",
    "    return root_mean_squared_log_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trial 0 achieved value: 0.39036500808754104\n",
      "Trial 1 achieved value: 0.37225424496295373 with  4.8652% improvement\n",
      "Trial 2 achieved value: 0.24255362040412584 with  53.4730% improvement\n",
      "Trial 3 achieved value: 0.24073260242724306 with  0.7564% improvement\n",
      "Mean Absolute Error (MAE): 6024.815058328542\n",
      "Mean Squared Error (MSE): 89225913.49602105\n",
      "Root Mean Squared Error (RMSE): 9445.946934851003\n",
      "R-squared (Coefficient of Determination): 0.8573293410739671\n",
      "Root Mean Squared Log Error (RMSLE): 0.2409493261592737 - Metric used in competition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/21 19:07:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, callbacks=[champion_callback], timeout=7200) # set n_trials to higher when doing final training and summary\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train final model with best parameters\n",
    "experiment = mlflow.set_experiment(\"Bluebook_for_bulldozers_RandomForestRegressor\")\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "        # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Bluebook for Bulldozers\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"RandomForestRegressor\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    final_pipeline = Pipeline([\n",
    "        ('RandomForestRegressor', RandomForestRegressor(**best_params))\n",
    "    ])\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmsle = root_mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"RMSLE\": rmsle\n",
    "    })\n",
    "\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    print(f\"R-squared (Coefficient of Determination): {r2}\")\n",
    "    print(f\"Root Mean Squared Log Error (RMSLE): {rmsle} - Metric used in competition\")\n",
    "    mlflow.sklearn.log_model(final_pipeline, \"model\")\n",
    "\n",
    "    # Create and log visualization\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Actual vs Predicted')\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    actual_vs_predicted_path = \"actual_vs_predicted.png\"\n",
    "    plt.savefig(actual_vs_predicted_path)\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(actual_vs_predicted_path)\n",
    "    os.remove(actual_vs_predicted_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
