{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "while any(marker in os.getcwd() for marker in ('exercises', 'notebooks', 'students', 'research', 'projects')):\n",
    "    os.chdir(\"..\")\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\mda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import optuna\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_log_error # Metric used in the competition for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI to local directory\n",
    "mlflow.set_tracking_uri(\"projects/proj_2_team_4/mlruns\")\n",
    "\n",
    "# Use for running ui:\n",
    "# mlflow ui --backend-store-uri \"Absolute path\"\n",
    "# e.g.: mlflow ui --backend-store-uri \"/Users/wojciechjurewicz/Desktop/Multivariate Data Analysis/Lab/mda2425/projects/proj_2_team_4/mlruns\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from the correct .env file location\n",
    "env_path = 'projects/proj_2_team_4/.env'\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset path from environment variable\n",
    "train_preprocessed_path = os.getenv('TRAIN_PREPROCESSED_PATH')\n",
    "test_preprocessed_path = os.getenv('VALID_PREPROCESSED_PATH')\n",
    "\n",
    "df_train = pd.read_csv(\"projects/proj_2_team_4/datasets/Train_preprocessed.csv\")\n",
    "df_test = pd.read_csv(\"projects/proj_2_team_4/datasets/Valid_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"SalePrice\"])\n",
    "y_train = df_train[\"SalePrice\"]\n",
    "\n",
    "X_test = df_test.drop(columns=[\"SalePrice\"])\n",
    "y_test = df_test[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 200\n",
    "learning_rate = 0.1\n",
    "max_depth = 3\n",
    "subsample = 1.0\n",
    "max_features = None\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('gbr', GradientBoostingRegressor(\n",
    "        n_estimators = n_estimators,\n",
    "        learning_rate = learning_rate,\n",
    "        max_depth = max_depth,\n",
    "        subsample = subsample,\n",
    "        max_features = max_features,\n",
    "        min_samples_split = min_samples_split,\n",
    "        min_samples_leaf = min_samples_leaf,\n",
    "        random_state = random_state\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "  \"\"\"\n",
    "  Logging callback that will report when a new trial iteration improves upon existing\n",
    "  best trial values.\n",
    "\n",
    "  Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "  or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "  workers or agents.\n",
    "  The race conditions with file system state management for distributed trials will render\n",
    "  inconsistent values with this callback.\n",
    "  \"\"\"\n",
    "\n",
    "  winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "  if study.best_value and winner != study.best_value:\n",
    "      study.set_user_attr(\"winner\", study.best_value)\n",
    "      if winner:\n",
    "          improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "          print(\n",
    "              f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "              f\"{improvement_percent: .4f}% improvement\"\n",
    "          )\n",
    "      else:\n",
    "          print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Create and train pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('gbr', GradientBoostingRegressor(**params))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    y_pred = np.maximum(0, y_pred) # Added to fix RMSLE ValueError\n",
    "    \n",
    "    # Return RMSLE as optimization metric\n",
    "    return root_mean_squared_log_error(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trial 0 achieved value: 0.3445053328174663\n",
      "Trial 1 achieved value: 0.33277312411179955 with  3.5256% improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/22 21:31:15 INFO mlflow.tracking.fluent: Experiment with name 'GradientBoostingRegressor_for_Bulldozers' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 achieved value: 0.3304285024454855 with  0.7096% improvement\n",
      "Mean Absolute Error (MAE): 8505.443522270594\n",
      "Mean Squared Error (MSE): 166611896.95110556\n",
      "Root Mean Squared Error (RMSE): 12907.823091098884\n",
      "R-squared (Coefficient of Determination): 0.7335905210542838\n",
      "Root Mean Squared Log Error (RMSLE): 0.3274962593599146 - Metric used in competition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/22 21:45:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# Create study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=3, callbacks=[champion_callback], timeout=7200) # set n_trials to higher when doing final training and summary\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train final model with best parameters\n",
    "experiment = mlflow.set_experiment(\"GradientBoostingRegressor_for_Bulldozers\")\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "# Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Bluebook for Bulldozers\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"GradientBoostingRegressor\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    final_pipeline = Pipeline([\n",
    "        ('gbr', GradientBoostingRegressor(**best_params))\n",
    "    ])\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmsle = root_mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"RMSLE\": rmsle\n",
    "    })\n",
    "\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    print(f\"R-squared (Coefficient of Determination): {r2}\")\n",
    "    print(f\"Root Mean Squared Log Error (RMSLE): {rmsle} - Metric used in competition\")\n",
    "    mlflow.sklearn.log_model(final_pipeline, \"model\")\n",
    "\n",
    "    # Create and log visualization\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Actual vs Predicted')\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    actual_vs_predicted_path = \"actual_vs_predicted.png\"\n",
    "    plt.savefig(actual_vs_predicted_path)\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(actual_vs_predicted_path)\n",
    "    os.remove(actual_vs_predicted_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
