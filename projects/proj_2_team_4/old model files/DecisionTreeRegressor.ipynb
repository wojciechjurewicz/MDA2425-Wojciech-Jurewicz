{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "while any(marker in os.getcwd() for marker in ('exercises', 'notebooks', 'students', 'research', 'projects')):\n",
    "    os.chdir(\"..\")\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow.sklearn\n",
    "import optuna\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "import time \n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ad943",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = '/Users/alanmakowski1/Desktop/project2/.env'\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"/Users/alanmakowski1/Desktop/visual_studio_code/mda2425/mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fd9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed_path = os.getenv('TRAIN_PREPROCESSED_LOG_PATH')\n",
    "test_preprocessed_path = os.getenv('VALID_PREPROCESSED_PATH')\n",
    "\n",
    "df_train = pd.read_csv(train_preprocessed_path)\n",
    "df_test = pd.read_csv(test_preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"SalePrice\"])\n",
    "y_train = df_train[\"SalePrice\"]\n",
    "X_test = df_test.drop(columns=[\"SalePrice\"])\n",
    "y_test = df_test[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "  \"\"\"\n",
    "  Logging callback that will report when a new trial iteration improves upon existing\n",
    "  best trial values.\n",
    "\n",
    "  Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "  or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "  workers or agents.\n",
    "  The race conditions with file system state management for distributed trials will render\n",
    "  inconsistent values with this callback.\n",
    "  \"\"\"\n",
    "\n",
    "  winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "  if study.best_value and winner != study.best_value:\n",
    "      study.set_user_attr(\"winner\", study.best_value)\n",
    "      if winner:\n",
    "          improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "          print(\n",
    "              f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "              f\"{improvement_percent: .4f}% improvement\"\n",
    "          )\n",
    "      else:\n",
    "          print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2887137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'friedman_mse']), \n",
    "        'splitter': 'best', \n",
    "        'max_depth': trial.suggest_int('max_depth', 8, 20),  \n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 8, 20),  \n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 4, 10),  \n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),  \n",
    "}\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('DecisionTree', DecisionTreeRegressor(**params))\n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X_train, np.log1p(y_train))  # log-transform target\n",
    "        y_pred_log = pipeline.predict(X_test)\n",
    "        y_pred = np.expm1(y_pred_log)  # inverse transform\n",
    "\n",
    "        duration = time.time() - start\n",
    "        print(f\"Trial {trial.number} took {duration:.2f} seconds.\")\n",
    "\n",
    "        return root_mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed with error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=1, callbacks=[champion_callback], show_progress_bar=True, \n",
    "               catch=(Exception,), timeout=1*60*60)\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "# === Start MLflow experiment ===\n",
    "experiment = mlflow.set_experiment(\"DecisionTreeRegressor_Bulldozers\")\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Bluebook for Bulldozers\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"DecisionTreeRegressor\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "    # Train final model\n",
    "    final_pipeline = Pipeline([\n",
    "        ('DecisionTreeRegressor', DecisionTreeRegressor(**best_params))\n",
    "    ])\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmsle = root_mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"RMSLE\": rmsle\n",
    "           \n",
    "    })\n",
    "\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(f\"RMSLE Score: {rmsle:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    mlflow.sklearn.log_model(final_pipeline, \"model\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Actual vs Predicted (DecisionTreeRegressor)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = \"actual_vs_predicted.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Log the plot\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    os.remove(plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
