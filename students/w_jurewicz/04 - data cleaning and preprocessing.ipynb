{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We shall use X to present on how do we use python tools to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset from seaborn\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "Lets check the dataset for missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Handling Missing Values\n",
    "\n",
    "### Dropping Missing Values\n",
    "\n",
    "We could drop a column deck since it has so many empty values. We would create a custom proper transformer to do so\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop specified columns\n",
    "        X_transformed = X.drop(columns=self.columns, axis=1)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DropColumnTransformer(columns=[\"deck\"]).fit_transform(df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Imputation Techniques (e.g., mean, median, mode)\n",
    "\n",
    "We may also use a predefined transformers in sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer to specify imputation for specific columns\n",
    "# In this example, we specify imputation for the 'chosen_column' using SimpleImputer with strategy='mean'\n",
    "# and 'other_column' without imputation\n",
    "\n",
    "\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy=\"mean\", columns: Optional[List[str]] = None):\n",
    "        self.strategy = strategy\n",
    "        self.columns = columns if columns is not None else []\n",
    "        self.imputer = SimpleImputer(strategy=self.strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.columns] = self.imputer.transform(X[self.columns])\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "CustomImputer(strategy=\"mean\", columns=[\"age\"]).fit_transform(df).age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Handling Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Identifying and Removing Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_duplicates = FunctionTransformer(lambda X: X.drop_duplicates(), validate=False)\n",
    "\n",
    "f\"Dataset at begining {len(df)} after drop {len(drop_duplicates.fit_transform(df))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Handling Outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Identifying Outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers\n",
    "\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "z_scores = stats.zscore(df[numerical_cols])\n",
    "threshold = 3\n",
    "outliers = (abs(z_scores) > threshold).any(axis=1)\n",
    "df[outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 \n",
    "\n",
    "Create a custom transformer to detect and remove outliers. Make threshold its parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemoveTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "        self.means = X[self.numerical_cols].mean()\n",
    "        self.stds = X[self.numerical_cols].std()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_num = X[self.numerical_cols]\n",
    "        z_scores = ((X_num - self.means) / self.stds).abs()\n",
    "        outliers = (z_scores > self.threshold).any(axis=1)\n",
    "        return X[~outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline construction\n",
    "\n",
    "We can create a pipeline to process the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning = make_pipeline(\n",
    "    DropColumnTransformer(columns=[\"deck\"]),\n",
    "    CustomImputer(strategy=\"mean\", columns=[\"age\"]),\n",
    "    CustomImputer(strategy=\"most_frequent\", columns=[\"embarked\"]),\n",
    "    FunctionTransformer(lambda X: X.drop_duplicates(), validate=False),\n",
    "    OutlierRemoveTransformer(threshold=3)\n",
    ")\n",
    "\n",
    "df_cleaned = data_cleaning.fit_transform(df)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Encoding Categorical Variables\n",
    "\n",
    "Most commonly we need to encode an object typed columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.select_dtypes(include=[\"object\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Label Encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns: List[str]) -> None:\n",
    "        self.columns = columns\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            self.encoders[column] = LabelEncoder()\n",
    "            self.encoders[column].fit(X[column])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            X_transformed[column] = self.encoders[column].transform(X[column])\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "CustomLabelEncoder(columns=[\"embarked\", \"embark_town\"]).fit_transform(df_cleaned).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "We shall use one hot for column with sex and who\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class CustomOneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns: List[str]) -> None:\n",
    "        self.columns = columns\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            self.encoders[column] = OneHotEncoder(sparse_output=False)\n",
    "            self.encoders[column].fit(X[[column]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            encoded = pd.DataFrame(\n",
    "                self.encoders[column].transform(X[[column]]),\n",
    "                columns=self.encoders[column].get_feature_names_out([column]),\n",
    "                index=X.index,\n",
    "            )\n",
    "            X_transformed = pd.concat(\n",
    "                [X_transformed.drop(columns=column), encoded], axis=1\n",
    "            )\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "CustomOneHotEncoder(columns=[\"sex\", \"who\"]).fit_transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Ordinal Encoding\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Create your own custom Encoder that uses OrdinalEncoder instead of LabelEncoder. Allow it to pass an order as a parameter\n",
    "\n",
    "for encoder relate to \n",
    "\n",
    "```python \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrdinalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns: List[str], order: Optional[dict] = None) -> None:\n",
    "        self.columns = columns\n",
    "        self.order = order if order is not None else {}\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            if column in self.order:\n",
    "                self.encoders[column] = OrdinalEncoder(categories=[self.order[column]])\n",
    "            else:\n",
    "                self.encoders[column] = OrdinalEncoder()\n",
    "            self.encoders[column].fit(X[[column]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            X_transformed[column] = self.encoders[column].transform(X[[column]])\n",
    "        return X_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scaling Numerical Features\n",
    "\n",
    "\n",
    "\n",
    "### Standardization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns: List[str]) -> None:\n",
    "        self.columns = columns\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.columns] = self.scaler.transform(X[self.columns])\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "CustomStandardScaler(columns=[\"fare\", \"age\"]).fit_transform(df_cleaned).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Min-Max Scaling\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Create a custom transformer with min-max scalling. Check\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMinMaxScaler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns: List[str]) -> None:\n",
    "        self.columns = columns\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.columns] = self.scaler.transform(X[self.columns])\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 \n",
    "\n",
    "Create a transformer to replace target. You may search for a thing called TargetEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline\n",
    "\n",
    "we now can create a preprocessing pipeline. It requires to provide an initial split for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_cleaned.drop(columns=[\"alone\", \"alive\"]),\n",
    "    df_cleaned[\"alive\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = make_pipeline(\n",
    "    CustomLabelEncoder(columns=[\"embarked\", \"embark_town\"]),\n",
    "    CustomOneHotEncoder(columns=[\"sex\", \"who\"]),\n",
    "    CustomStandardScaler(columns=[\"fare\", \"age\"]),\n",
    "    CustomOrdinalEncoder(columns=[\"class\"], order={\"class\": [\"First\", \"Second\", \"Third\"]}),\n",
    "    CustomMinMaxScaler(columns=[\"sibsp\", \"parch\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = preprocessing_pipeline.transform(X_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass-Fail Exercise \n",
    "\n",
    "Complete the exercises presented in this notebook. Then copy this notebook to your student directory and create a Merge request with it. Please do not commit thios file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
