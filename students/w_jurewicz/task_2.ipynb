{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop specified columns\n",
    "        X_transformed = X.drop(columns=self.columns, axis=1)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrdinalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns: List[str], order: Optional[dict] = None) -> None:\n",
    "        self.columns = columns\n",
    "        self.order = order if order is not None else {}\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            if column in self.order:\n",
    "                self.encoders[column] = OrdinalEncoder(categories=[self.order[column]])\n",
    "            else:\n",
    "                self.encoders[column] = OrdinalEncoder()\n",
    "            self.encoders[column].fit(X[[column]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            X_transformed[column] = self.encoders[column].transform(X[[column]])\n",
    "        return X_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemoveTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.outlier_indices = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "        z_scores = stats.zscore(X[numerical_cols])\n",
    "        self.outlier_indices = (abs(z_scores) > self.threshold).any(axis=1)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[~self.outlier_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns: List[str]) -> None:\n",
    "        self.columns = columns\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            self.encoders[column] = LabelEncoder()\n",
    "            self.encoders[column].fit(X[column])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            X_transformed[column] = self.encoders[column].transform(X[column])\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns: List[str]) -> None:\n",
    "        self.columns = columns\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            self.encoders[column] = OneHotEncoder(sparse_output=False)\n",
    "            self.encoders[column].fit(X[[column]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            encoded = pd.DataFrame(\n",
    "                self.encoders[column].transform(X[[column]]),\n",
    "                columns=self.encoders[column].get_feature_names_out([column]),\n",
    "                index=X.index,\n",
    "            )\n",
    "            X_transformed = pd.concat(\n",
    "                [X_transformed.drop(columns=column), encoded], axis=1\n",
    "            )\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns: List[str]) -> None:\n",
    "        self.columns = columns\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.columns] = self.scaler.transform(X[self.columns])\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineeringTransformer:\n",
    "    def __init__(self, new_column, transformation):\n",
    "        self.new_column = new_column\n",
    "        self.transformation = transformation\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        X_transformed[self.new_column] = self.transformation(X)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube comments dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"atifaliak/youtube-comments-dataset\")\n",
    "\n",
    "youtube_comments_df = pd.read_csv(os.path.join(path, 'YoutubeCommentsDataSet.csv'))\n",
    "\n",
    "youtube_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(youtube_comments_df.shape)\n",
    "print(youtube_comments_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "youtube_comments_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_comments_df_cleaning = make_pipeline(\n",
    "    FunctionTransformer(lambda X: X.drop_duplicates(), validate=False)\n",
    ")\n",
    "\n",
    "youtube_comments_df_cleaned = youtube_comments_df_cleaning.fit_transform(youtube_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_comments_preprocessing_pipeline = make_pipeline(\n",
    "    CustomOrdinalEncoder(columns=[\"Sentiment\"], order={\"Sentiment\": [\"negative\", \"neutral\", \"positive\"]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_comments_df_preprocessed = youtube_comments_preprocessing_pipeline.fit_transform(youtube_comments_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_comments_df_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"shubhambathwal/flight-price-prediction\")\n",
    "\n",
    "flight_price_df = pd.read_csv(os.path.join(path, 'Clean_Dataset.csv'))\n",
    "\n",
    "flight_price_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flight_price_df.shape)\n",
    "print(flight_price_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_price_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_price_df_cleaning = make_pipeline(\n",
    "    DropColumnTransformer(columns=[\"Unnamed: 0\"]),\n",
    "    OutlierRemoveTransformer(threshold=3)\n",
    ")\n",
    "\n",
    "flight_price_df_cleaned = flight_price_df_cleaning.fit_transform(flight_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_price_preprocessing_pipeline = make_pipeline(\n",
    "    CustomOneHotEncoder(columns=[\"airline\"]),\n",
    "    CustomOrdinalEncoder(columns=[\"departure_time\", \"stops\", \"arrival_time\", \"class\"], order={\"departure_time\": [\"Early_Morning\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\", \"Late_Night\"], \"stops\": [\"zero\", \"one\", \"two_or_more\"], \"arrival_time\": [\"Early_Morning\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\", \"Late_Night\"], \"class\": [\"Economy\", \"Business\"]}),\n",
    "    CustomLabelEncoder(columns=[\"source_city\", \"destination_city\", \"flight\"]),\n",
    "    CustomStandardScaler(columns=[\"duration\", \"price\", \"days_left\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_price_df_preprocessed = flight_price_preprocessing_pipeline.fit_transform(flight_price_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_price_df_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"arjunbhasin/credit-card-dataset\")\n",
    "\n",
    "credit_card_df = pd.read_csv(os.path.join(path, 'credit_data_norm.csv'))\n",
    "\n",
    "credit_card_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_card_df.shape)\n",
    "print(credit_card_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_cleaning = make_pipeline(\n",
    "    DropColumnTransformer(columns=[\"itzsv_minimum_payments_14\"]),\n",
    "    OutlierRemoveTransformer(threshold=3)\n",
    ")\n",
    "\n",
    "credit_card_df_cleaned = credit_card_df_cleaning.fit_transform(credit_card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_preprocessing_pipeline = make_pipeline(\n",
    "    FeatureEngineeringTransformer(\"purchases_ratio\", lambda X: X[\"pwnjx_purchases_2\"] / (X[\"xslth_balance_0\"])),\n",
    "    FeatureEngineeringTransformer(\"installments_ratio\", lambda X: X[\"ojukq_installments_purchases_4\"] / (X[\"pwnjx_purchases_2\"])),\n",
    "    FeatureEngineeringTransformer(\"cash_advance_usage_ratio\", lambda X: X[\"bvnag_cash_advance_5\"] / (X[\"xslth_balance_0\"])),\n",
    "    FeatureEngineeringTransformer(\"balance_frequency_percent\", lambda X: X[\"fmeyv_balance_frequency_1\"]),\n",
    "    FeatureEngineeringTransformer(\"prc_full_payment_percent\", lambda X: X[\"ubvma_prc_full_payment_15\"])\n",
    ")\n",
    "\n",
    "credit_card_df_preprocessed = credit_card_preprocessing_pipeline.fit_transform(credit_card_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_preprocessed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
