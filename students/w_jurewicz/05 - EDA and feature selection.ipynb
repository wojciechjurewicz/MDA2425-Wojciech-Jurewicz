{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "while any(marker in os.getcwd() for marker in ('exercises', 'notebooks', 'students', 'research')):\n",
    "    os.chdir(\"..\")\n",
    "sys.path.append('src')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from src.custom_transformers import (\n",
    "    DropColumnTransformer,\n",
    "    CustomImputer,\n",
    "    CustomStandardScaler,\n",
    "    CustomLabelEncoder,\n",
    "    CustomOneHotEncoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset from seaborn\n",
    "raw_data = sns.load_dataset(\"titanic\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import stats\n",
    "\n",
    "class OutlierRemoveTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "        self.means = X[self.numerical_cols].mean()\n",
    "        self.stds = X[self.numerical_cols].std()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_num = X[self.numerical_cols]\n",
    "        z_scores = ((X_num - self.means) / self.stds).abs()\n",
    "        outliers = (z_scores > self.threshold).any(axis=1)\n",
    "        return X[~outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning = make_pipeline(\n",
    "    DropColumnTransformer(columns=[\"deck\"]),\n",
    "    CustomImputer(strategy=\"mean\", columns=[\"age\"]),\n",
    "    CustomImputer(strategy=\"most_frequent\", columns=[\"embarked\"]),\n",
    "    FunctionTransformer(lambda X: X.drop_duplicates(), validate=False),\n",
    "    OutlierRemoveTransformer(threshold=3)\n",
    ")\n",
    "\n",
    "df_cleaned = data_cleaning.fit_transform(raw_data)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_cleaned.drop(columns=[\"alive\", \"survived\"]),\n",
    "    df_cleaned[\"alive\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = make_pipeline(\n",
    "    CustomLabelEncoder(columns=[\"embarked\", \"embark_town\"]),\n",
    "    CustomOneHotEncoder(columns=[\"sex\", \"who\", \"adult_male\", \"class\"]),\n",
    "    CustomStandardScaler(columns=[\"fare\", \"age\"]),\n",
    "    CustomLabelEncoder(columns=[\"alone\"])\n",
    ")\n",
    "df = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE*\n",
    "\n",
    "You should never use test dataset for exploratory analysis. Otherwise you would have no place to check you observations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA \n",
    "\n",
    "Exploratory Data Analysis is an initial step when working with new dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Statistics\n",
    "\n",
    "We start by providing some general properties of our dataset. It is also good to provide some on raw_data we read from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_statistics(df: pd.DataFrame, y: pd.Series):\n",
    "\n",
    "    return {\n",
    "        \"describe\": df.describe(),\n",
    "        \"y\": y.value_counts(dropna=False) / len(y),\n",
    "        \"datatypes\": df.info(verbose=True),\n",
    "    }\n",
    "\n",
    "\n",
    "print(f'Raw: {sample_statistics(raw_data.drop(columns=[\"alive\"]), raw_data[\"alive\"])}')\n",
    "print(f\"Preprocessed {sample_statistics(df, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variability(df):\n",
    "    # Compute the variance of each column\n",
    "    variances = df.var()\n",
    "\n",
    "    # Compute the mean value of each column\n",
    "    means = df.mean()\n",
    "\n",
    "    # Scale the variance by the mean value of each feature\n",
    "    scaled_variances = variances / means\n",
    "\n",
    "    # Create a DataFrame to store the scaled variances\n",
    "    variability_df = pd.DataFrame(\n",
    "        {\"Feature\": scaled_variances.index, \"Variability\": scaled_variances.values}\n",
    "    )\n",
    "\n",
    "    return variability_df\n",
    "\n",
    "\n",
    "variability(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Write a function that would do following things:\n",
    "* detect column with nulls\n",
    "* detect constant columns\n",
    "* detect columns with unique values but also ensuring column is not floating point value. \n",
    "It is a good idea to return results as dictionary with keys like 'null_cols', 'const_cols', etc and values being lists with string-typed column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_columns(df: pd.DataFrame) -> dict:\n",
    "    null_cols = df.columns[df.isnull().any()].tolist()\n",
    "    \n",
    "    const_cols = [col for col in df.columns if len(df[col].unique())==1]\n",
    "    \n",
    "    unique_cols = [col for col in df.columns if df[col].nunique() == len(df) and not pd.api.types.is_float_dtype(df[col])]\n",
    "    \n",
    "    return {\n",
    "        \"null_cols\": null_cols,\n",
    "        \"const_cols\": const_cols,\n",
    "        \"unique_cols\": unique_cols,\n",
    "    }\n",
    "\n",
    "print(detect_columns(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Titanic Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Write a code that instead of displaying correlation matrix would store results in dataframe, possibly sroted by absolute values of correlations. It would be best to return a dataframe with correlation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_correlation_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    correlation_series = correlation_matrix.unstack()\n",
    "\n",
    "    correlation_df = correlation_series.reset_index()\n",
    "    correlation_df.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "\n",
    "    correlation_df['Abs_Correlation'] = correlation_df['Correlation'].abs()\n",
    "    sorted_correlation_df = correlation_df.sort_values(by='Abs_Correlation', ascending=False)\n",
    "\n",
    "    return sorted_correlation_df\n",
    "\n",
    "sorted_correlation_df = sorted_correlation_matrix(df)\n",
    "sorted_correlation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df, y_train], axis=1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot with hue set to the target variable\n",
    "sns.pairplot(df_combined, hue=\"alive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The plot is hard to read. Write a code to split into two pieces. Mind so the the both plots would contain target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_group_1 = list(df.columns[:len(df.columns) // 2])\n",
    "features_group_2 = list(df.columns[len(df.columns) // 2:])\n",
    "\n",
    "sns.pairplot(df_combined, vars=features_group_1, hue=\"alive\")\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(df_combined, vars=features_group_2, hue=\"alive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Write a code to plot histograms separately. Its is a good idea to keep them separated by colour though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data=df_combined, x=column, kde=True, hue=\"alive\", multiple=\"layer\")\n",
    "    plt.title(f\"Histogram of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform feature selection using feature importance\n",
    "selector = SelectFromModel(clf)\n",
    "selector.fit(df, y_train)\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = df.columns[selected_features_indices]\n",
    "\n",
    "# Create DataFrame with selected features\n",
    "df_selected = df[selected_features]\n",
    "\n",
    "# Display selected features\n",
    "print(\"Selected Features:\")\n",
    "print(df_selected.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression model with Lasso regularization\n",
    "logistic_lasso = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_lasso.fit(df, y_train)\n",
    "\n",
    "# Get coefficients of the model\n",
    "coefficients = logistic_lasso.coef_\n",
    "\n",
    "# Get indices of non-zero coefficients (i.e., selected features)\n",
    "selected_feature_indices = (coefficients != 0).flatten()\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = df.columns[selected_feature_indices]\n",
    "\n",
    "# Create DataFrame with selected features\n",
    "df_selected = df[selected_features]\n",
    "\n",
    "# Display selected features\n",
    "print(\"Selected Features:\")\n",
    "print(df_selected.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 \n",
    "\n",
    "Write couple of statistical test to explore its properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "# T-test for comparing the means of fare between alive and not alive\n",
    "alive_yes = df_combined[df_combined['alive'] == 'yes']['fare']\n",
    "alive_no = df_combined[df_combined['alive'] == 'no']['fare']\n",
    "t_stat, p_value = ttest_ind(alive_yes, alive_no, nan_policy='omit')\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference in the mean of fare between alive and not alive passangers.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in the mean fare between alive and not alive passangers.\")\n",
    "\n",
    "# Chi-square test for independence between pclass and alive\n",
    "contingency_table = pd.crosstab(df_combined['pclass'], df_combined['alive'])\n",
    "chi2_stat, chi2_p_value, _, _ = chi2_contingency(contingency_table)\n",
    "if chi2_p_value < 0.05:\n",
    "    print(\"There is a significant association between pclass and alive\")\n",
    "else:\n",
    "    print(\"There is no significant association between pclass and alive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 \n",
    "\n",
    "Plot each feature in function of index. This would let you observe whether variance in feature is uniformly distributed (So the data ain't ordered by anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.scatter(df.index, df[column], alpha=0.3)\n",
    "    plt.title(f\"{column} vs Index\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass-Fail Exercise \n",
    "\n",
    "Complete the exercises presented in this notebook. Then copy this notebook to your student directory and create a Merge request with it. Please do not commit thios file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
